{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I see four methods to store internal state:\n",
    "\n",
    "* Storing state:\n",
    "    * centralized\n",
    "    * decentralized\n",
    "* Communication\n",
    "    * asynchronous\n",
    "    * synchronous\n",
    "\n",
    "Here's what these two decisions mean:\n",
    "\n",
    "1. Asynchronous, centralized (e.g., [Hogwild](https://arxiv.org/abs/1106.5730))\n",
    "    * Will require holding one model vector on one parameter server\n",
    "    * Will mean that workers pull/push this model at any time\n",
    "2. Synchronous, centralized (e.g., mini-batch SGD with one-to-all)\n",
    "    * Same as (1), but waits for model to be fully updated before workers can pull model\n",
    "3. Synchronous, decentralized (e.g., mini-batch SGD w/ all-reduce)\n",
    "    * Every worker decides how to communicate\n",
    "    * Every worker holds onto a model\n",
    "4. Asynchronous, decentralized (e.g., [Hogwild++](https://ieeexplore.ieee.org/abstract/document/7837887/))\n",
    "    * Not every worker communicates with every other worker\n",
    "    * e.g., a bunch of point communications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good reference of speeds up Paleo for minibatch SGD: https://talwalkarlab.github.io/paleo/ (and choose \"strong scaling\" instead of \"weak scaling\"). This shows that for mini-batch SGD **all-reduce is >5x faster than all-to-one.**\n",
    "\n",
    "* Centralized:\n",
    "    * pros: large models\n",
    "    * cons: slower, functions\n",
    "* Decentralized\n",
    "    * pros: faster, classes.\n",
    "    * advantage: decentralized is a superset of centralized (communication strategy can be customized).\n",
    "    * cons: large model\n",
    "* sync:\n",
    "    * pros: simple. no assumptions.\n",
    "    * cons: maybe slower?\n",
    "* async:\n",
    "    * pros: maybe faster\n",
    "    * cons: not simple. assumptions.\n",
    "    \n",
    "I'm inclined to go with decentralized + sync."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Related work\n",
    "* Ray is most similar to Dask. https://ray.readthedocs.io/en/latest/example-parameter-server.html\n",
    "* Horovod, an MPI wrapper with nice API: https://github.com/uber/horovod\n",
    "* Tensorflow parameter server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Client</h3>\n",
       "<ul>\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:52000\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Cluster</h3>\n",
       "<ul>\n",
       "  <li><b>Workers: </b>8</li>\n",
       "  <li><b>Cores: </b>8</li>\n",
       "  <li><b>Memory: </b>17.18 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: scheduler='tcp://127.0.0.1:52000' processes=8 cores=8>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from distributed import Client\n",
    "client = Client()\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sketches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asynchronous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PS:\n",
    "    def __init__(self):\n",
    "        self._store = {'model': 0}\n",
    "        \n",
    "    def get(self, key):\n",
    "        return self._store[key]\n",
    "    \n",
    "    def set(self, key, value, i=0):\n",
    "        self._store[key] += value\n",
    "    \n",
    "    def store(self):\n",
    "        return self._store\n",
    "\n",
    "def update(ps, i=0):\n",
    "    for _ in range(4):\n",
    "        model = ps.get('model').result()\n",
    "        new_model = model + 1\n",
    "        ps.set('model', new_model, i=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Actor: PS, key=PS-e4f5ac0b-4001-49de-bef6-fa43bf2faf71>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "futures = client.submit(PS, actor=True)\n",
    "ps = client.gather(futures)\n",
    "ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None, None, None]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "futures = [client.submit(update, ps, i=i) for i in range(4)]\n",
    "client.gather(futures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 5759}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.store().result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synchronous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distributed import Client\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Worker:\n",
    "    def __init__(self, model, n_models, worker_id):\n",
    "        self.model = model\n",
    "        self.grads = []\n",
    "        self.n_models = n_models\n",
    "        self.worker_id = worker_id\n",
    "        \n",
    "    def _model(self):\n",
    "        return self.model\n",
    "    \n",
    "    def compute(self):\n",
    "        self.grad = self.worker_id\n",
    "        self.grads += [self.grad]\n",
    "        return True\n",
    "    \n",
    "    def send(self, worker):\n",
    "        worker.recv(self.grad)\n",
    "        \n",
    "    def recv(self, grad):\n",
    "        self.grads += [grad]\n",
    "    \n",
    "    def reduce(self):\n",
    "        assert len(self.grads) == 4\n",
    "        self.model += sum(self.grads)\n",
    "        self.grads = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Actor: Worker, key=Worker-33a35517-bb78-4708-9eb0-5340eb6978b7>,\n",
       " <Actor: Worker, key=Worker-aaf61086-c506-45d4-b4ee-f8dfdc27c75c>,\n",
       " <Actor: Worker, key=Worker-4af885e4-0652-4dac-abb8-aad458beeaca>,\n",
       " <Actor: Worker, key=Worker-d881d528-2334-42ae-a12b-b2be311e3026>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = 0\n",
    "n_models = 4\n",
    "futures = [client.submit(Worker, model, n_models, i, actor=True) \n",
    "           for i in range(n_models)]\n",
    "workers = client.gather(futures)\n",
    "workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result of client.gather() are ActorFutures? [<ActorFuture>, <ActorFuture>, <ActorFuture>, <ActorFuture>]\n",
      "models = [54, 54, 54, 54]\n",
      "models = [60, 60, 60, 60]\n",
      "models = [66, 66, 66, 66]\n",
      "models = [72, 72, 72, 72]\n"
     ]
    }
   ],
   "source": [
    "for k in range(4):\n",
    "    # calculate\n",
    "    futures = [worker.compute() for worker in workers]\n",
    "    client.gather(futures)\n",
    "\n",
    "    # communicate\n",
    "    # (updating model hapepns internally; worker knows when fully received model)\n",
    "    # (this could be an all-reduce implementation if desired)\n",
    "    futures = []\n",
    "    for i, w1 in enumerate(workers):\n",
    "        for j, w2 in enumerate(workers):\n",
    "            if i == j:\n",
    "                continue\n",
    "            else:\n",
    "                futures += [w1.send(w2)]\n",
    "    client.gather(futures)\n",
    "\n",
    "    # update model\n",
    "    futures = [worker.reduce() for worker in workers]\n",
    "    client.gather(futures)\n",
    "\n",
    "    # quick test; make sure all models are the same\n",
    "    futures = [worker._model() for worker in workers]\n",
    "    if k == 0:\n",
    "        print(\"result of client.gather() are ActorFutures?\", client.gather(futures))\n",
    "        \n",
    "    models = [m.result() for m in futures]\n",
    "    print(\"models =\", models)\n",
    "    assert all(model == models[0] for model in models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<ActorFuture>, <ActorFuture>, <ActorFuture>, <ActorFuture>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads = [worker.compute() for worker in workers]\n",
    "grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'int' and 'ActorFuture'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-f4434426b001>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'int' and 'ActorFuture'"
     ]
    }
   ],
   "source": [
    "sum(grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
